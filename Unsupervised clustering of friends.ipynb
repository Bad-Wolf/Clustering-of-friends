{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised clustering of friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "import transliterate\n",
    "from transliterate.contrib.languages.bg.translit_language_pack import BulgarianLanguagePack\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bulstem import stem\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ME = \"–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(filepath, friend_name=True):\n",
    "    \"\"\"\n",
    "    Reads facebook messenger's JSON file and returns a pandas Dataframe.\n",
    "    \n",
    "    Doesn't return a dataframe if the participants are more\n",
    "    than two people(no group chats).\n",
    "    \n",
    "    Works only on the \"Messages\" json files downloaded through\n",
    "    Facebook's \"Download Your Information\" section.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : string\n",
    "        Filepath to the JSON file.\n",
    "    friend_name : boolean, default True\n",
    "        If True, adds an aditional column \"friend_name\" to the df.\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataframe        \n",
    "    \"\"\"\n",
    "    # Fixes bad encoding\n",
    "    fix_mojibake_escapes = partial(re.compile(rb'\\\\u00([\\da-f]{2})').sub, lambda m: bytes.fromhex(m.group(1).decode()))\n",
    "    \n",
    "    # Need to read as binary to decode correctly\n",
    "    with open(filepath, 'rb') as file:    \n",
    "        repaired = fix_mojibake_escapes(file.read())\n",
    "        data = json.loads(repaired.decode('utf8'), strict=False)\n",
    "        \n",
    "        # No group chats!\n",
    "        if len(data['participants']) == 2:\n",
    "            result = pd.DataFrame.from_dict(data['messages'])\n",
    "            \n",
    "            # Additional column\n",
    "            if friend_name:\n",
    "                participants = pd.Series(data['participants']).apply(pd.Series)\n",
    "                for name in participants.name:\n",
    "                    if not name == ME:\n",
    "                        result['friend_name'] = name\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"messages/inbox/*/message_1.json\")\n",
    "data = pd.concat((json_to_df(filename) for filename in all_files), ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sender_name</th>\n",
       "      <th>share</th>\n",
       "      <th>type</th>\n",
       "      <th>friend_name</th>\n",
       "      <th>photos</th>\n",
       "      <th>sticker</th>\n",
       "      <th>audio_files</th>\n",
       "      <th>gifs</th>\n",
       "      <th>reactions</th>\n",
       "      <th>videos</th>\n",
       "      <th>files</th>\n",
       "      <th>call_duration</th>\n",
       "      <th>missed</th>\n",
       "      <th>users</th>\n",
       "      <th>plan</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:15.928</th>\n",
       "      <td>–¥–æ–±—Ä–µ, —á–∞–∫–∞–π —Å–µ–≥–∞ –Ω–µ–¥–µ–π –¥–∞ –ø–∏—à–µ—à –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ –≤...</td>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:31.911</th>\n",
       "      <td>some English text 123 wohoo yeaaah !!! xD</td>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:36.887</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'uri': 'messages/stickers_used/39178562_15051...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:55.550</th>\n",
       "      <td>sh1okavica bum bam nqkvi dumi</td>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:10:12.635</th>\n",
       "      <td>–î–∞–Ω—á–æ –µ —Å—É–ø–µ—Ä –ø–∏—á üòé</td>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Generic</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   content  \\\n",
       "timestamp                                                                    \n",
       "2019-07-22 15:09:15.928  –¥–æ–±—Ä–µ, —á–∞–∫–∞–π —Å–µ–≥–∞ –Ω–µ–¥–µ–π –¥–∞ –ø–∏—à–µ—à –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ –≤...   \n",
       "2019-07-22 15:09:31.911          some English text 123 wohoo yeaaah !!! xD   \n",
       "2019-07-22 15:09:36.887                                                NaN   \n",
       "2019-07-22 15:09:55.550                      sh1okavica bum bam nqkvi dumi   \n",
       "2019-07-22 15:10:12.635                                –î–∞–Ω—á–æ –µ —Å—É–ø–µ—Ä –ø–∏—á üòé   \n",
       "\n",
       "                            sender_name share     type    friend_name photos  \\\n",
       "timestamp                                                                      \n",
       "2019-07-22 15:09:15.928  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   NaN  Generic  Daniel Petrov    NaN   \n",
       "2019-07-22 15:09:31.911  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   NaN  Generic  Daniel Petrov    NaN   \n",
       "2019-07-22 15:09:36.887   Daniel Petrov   NaN  Generic  Daniel Petrov    NaN   \n",
       "2019-07-22 15:09:55.550  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   NaN  Generic  Daniel Petrov    NaN   \n",
       "2019-07-22 15:10:12.635  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   NaN  Generic  Daniel Petrov    NaN   \n",
       "\n",
       "                                                                   sticker  \\\n",
       "timestamp                                                                    \n",
       "2019-07-22 15:09:15.928                                                NaN   \n",
       "2019-07-22 15:09:31.911                                                NaN   \n",
       "2019-07-22 15:09:36.887  {'uri': 'messages/stickers_used/39178562_15051...   \n",
       "2019-07-22 15:09:55.550                                                NaN   \n",
       "2019-07-22 15:10:12.635                                                NaN   \n",
       "\n",
       "                        audio_files gifs reactions videos files  \\\n",
       "timestamp                                                         \n",
       "2019-07-22 15:09:15.928         NaN  NaN       NaN    NaN   NaN   \n",
       "2019-07-22 15:09:31.911         NaN  NaN       NaN    NaN   NaN   \n",
       "2019-07-22 15:09:36.887         NaN  NaN       NaN    NaN   NaN   \n",
       "2019-07-22 15:09:55.550         NaN  NaN       NaN    NaN   NaN   \n",
       "2019-07-22 15:10:12.635         NaN  NaN       NaN    NaN   NaN   \n",
       "\n",
       "                         call_duration missed users plan  \n",
       "timestamp                                                 \n",
       "2019-07-22 15:09:15.928            NaN    NaN   NaN  NaN  \n",
       "2019-07-22 15:09:31.911            NaN    NaN   NaN  NaN  \n",
       "2019-07-22 15:09:36.887            NaN    NaN   NaN  NaN  \n",
       "2019-07-22 15:09:55.550            NaN    NaN   NaN  NaN  \n",
       "2019-07-22 15:10:12.635            NaN    NaN   NaN  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.timestamp_ms = pd.to_datetime(data.timestamp_ms, unit='ms')\n",
    "data = data.sort_values('timestamp_ms')\n",
    "data = data.drop_duplicates('timestamp_ms')\n",
    "data = data.set_index('timestamp_ms', verify_integrity=True)\n",
    "data.index.names = ['timestamp']\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = data[(data.type=='Generic') | (data.type=='Share')][['sender_name', 'content', 'friend_name', 'type', 'share', 'reactions']]\n",
    "mess.content.dropna(inplace=True)\n",
    "mess.sender_name = mess.sender_name.astype('category')\n",
    "mess.friend_name = mess.friend_name.astype('category')\n",
    "mess.type = mess.type.astype('category')\n",
    "mess.content = mess.content.str.lower()\n",
    "mess.content = mess.content.replace(np.nan, '0')\n",
    "mess = mess[mess.content!='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender_name</th>\n",
       "      <th>content</th>\n",
       "      <th>friend_name</th>\n",
       "      <th>type</th>\n",
       "      <th>share</th>\n",
       "      <th>reactions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:08:48.294</th>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>–∞–∫–æ –º–Ω –≥–æ –∑–∞–∫—ä—Å–∞—à —Å —Å—ä–±–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ —Å—ä–æ–±—â–µ–Ω–∏—è—Ç–∞ ...</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:15.928</th>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>–¥–æ–±—Ä–µ, —á–∞–∫–∞–π —Å–µ–≥–∞ –Ω–µ–¥–µ–π –¥–∞ –ø–∏—à–µ—à –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ –≤...</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:31.911</th>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>some english text 123 wohoo yeaaah !!! xd</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:09:55.550</th>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>sh1okavica bum bam nqkvi dumi</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-22 15:10:12.635</th>\n",
       "      <td>–ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤</td>\n",
       "      <td>–¥–∞–Ω—á–æ –µ —Å—É–ø–µ—Ä –ø–∏—á üòé</td>\n",
       "      <td>Daniel Petrov</td>\n",
       "      <td>Generic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sender_name  \\\n",
       "timestamp                                 \n",
       "2019-07-22 15:08:48.294   Daniel Petrov   \n",
       "2019-07-22 15:09:15.928  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   \n",
       "2019-07-22 15:09:31.911  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   \n",
       "2019-07-22 15:09:55.550  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   \n",
       "2019-07-22 15:10:12.635  –ü–∞–≤–µ–ª –ë–æ–≥–¥–∞–Ω–æ–≤   \n",
       "\n",
       "                                                                   content  \\\n",
       "timestamp                                                                    \n",
       "2019-07-22 15:08:48.294  –∞–∫–æ –º–Ω –≥–æ –∑–∞–∫—ä—Å–∞—à —Å —Å—ä–±–∏—Ä–∞–Ω–µ—Ç–æ –Ω–∞ —Å—ä–æ–±—â–µ–Ω–∏—è—Ç–∞ ...   \n",
       "2019-07-22 15:09:15.928  –¥–æ–±—Ä–µ, —á–∞–∫–∞–π —Å–µ–≥–∞ –Ω–µ–¥–µ–π –¥–∞ –ø–∏—à–µ—à –∑–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ –≤...   \n",
       "2019-07-22 15:09:31.911          some english text 123 wohoo yeaaah !!! xd   \n",
       "2019-07-22 15:09:55.550                      sh1okavica bum bam nqkvi dumi   \n",
       "2019-07-22 15:10:12.635                                –¥–∞–Ω—á–æ –µ —Å—É–ø–µ—Ä –ø–∏—á üòé   \n",
       "\n",
       "                           friend_name     type share reactions  \n",
       "timestamp                                                        \n",
       "2019-07-22 15:08:48.294  Daniel Petrov  Generic   NaN       NaN  \n",
       "2019-07-22 15:09:15.928  Daniel Petrov  Generic   NaN       NaN  \n",
       "2019-07-22 15:09:31.911  Daniel Petrov  Generic   NaN       NaN  \n",
       "2019-07-22 15:09:55.550  Daniel Petrov  Generic   NaN       NaN  \n",
       "2019-07-22 15:10:12.635  Daniel Petrov  Generic   NaN       NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "space_mess = mess.copy()\n",
    "space_mess.content = space_mess.content.str.split()\n",
    "space_mess.content.dropna(inplace=True)\n",
    "space_mess['wordcount_space'] = space_mess.content.apply(len)\n",
    "best_friends = space_mess.groupby('sender_name').wordcount_space.sum().nlargest(len(list(space_mess.sender_name.unique())))\n",
    "best_space_mess = space_mess[space_mess.friend_name.isin(list(best_friends[1:n+1].index))]\n",
    "best_space_mess.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transliteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "BulgarianLanguagePack.mapping = ('abwvgdejziyklmnoprstufhc461q', '–∞–±–≤–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—ä—è')\n",
    "\"\"\"\n",
    "my additions:\n",
    "    \"1\": \"—ä\",\n",
    "    \"6\": \"—à\",\n",
    "    \"4\": \"—á\",\n",
    "    \"q\": \"—è\",\n",
    "    \"w\": \"–≤\",\n",
    "    \"j\": \"–∂\",\n",
    "    \"c\": \"—Ü\",\n",
    "    \n",
    "removed all the uppercase characters for improved performance\n",
    "ABVWGDEZIYKLMNOPRSTUFHCQ:–ê–ë–í–í–ì–î–ï–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–Ø    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "BulgarianLanguagePack.reversed_specific_mapping = ('—å—ä', 'y1')\n",
    "\n",
    "\n",
    "BulgarianLanguagePack.pre_processor_mapping = {\n",
    "    'zh': '–∂',\n",
    "    'ts': '—Ü',\n",
    "    'ch': '—á',\n",
    "    'sh': '—à',\n",
    "    '—àt': '—â', # fixed\n",
    "    'yu': '—é',\n",
    "    'ya': '—è',\n",
    "    \n",
    "    # my additions:\n",
    "    \"6t\": \"—â\",\n",
    "    \"1o\": \"—å–æ\",    \n",
    "    \"1i\": \"—ä–π\",\n",
    "    \"ai\": \"–∞–π\",\n",
    "    # \"–π–æ\" only after vowel\n",
    "    # and beginning of word\n",
    "    \"yo\": \"–π–æ\",\n",
    "    \"b–π\": \"–±—å\",\n",
    "    \"v–π\": \"–≤—å\",\n",
    "    \"w–π\": \"–≤—å\",\n",
    "    \"g–π\": \"–≥—å\",\n",
    "    \"d–π\": \"–¥—å\",\n",
    "    \"j–π\": \"–∂—å\",\n",
    "    \"–∂–π\": \"–∂—å\",\n",
    "    \"z–π\": \"–∑—å\",\n",
    "    \"k–π\": \"–∫—å\",\n",
    "    \"l–π\": \"–ª—å\",\n",
    "    \"m–π\": \"–º—å\",\n",
    "    \"n–π\": \"–Ω—å\",\n",
    "    \"p–π\": \"–ø—å\",\n",
    "    \"r–π\": \"—Ä—å\",\n",
    "    \"s–π\": \"—Å—å\",\n",
    "    \"t–π\": \"—Ç—å\",\n",
    "    \"f–π\": \"—Ñ—å\",\n",
    "    \"c–π\": \"—Ü—å\",\n",
    "    \"—á–π\": \"—á—å\",\n",
    "    \"—à–π\": \"—à—å\",\n",
    "    \"—â–π\": \"—â—å\"}\n",
    "\"\"\"\n",
    "    removed:\n",
    "    'Zh': '–ñ',\n",
    "    'Ts': '–¶',\n",
    "    'Ch': '–ß',\n",
    "    'Sh': '–®',\n",
    "    '–®t': '–©', # fixed\n",
    "    'Yu': '–Æ',\n",
    "    'Ya': '–Ø',\n",
    "\"\"\"\n",
    "\n",
    "# Instead of callng the language code each time\n",
    "translit_bg = transliterate.get_translit_function('bg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_wordlist = pd.read_csv('en_words.csv', header=None, names=['word'])\n",
    "bg_wordlist = pd.read_csv('bg_full.txt', header=None, delim_whitespace=True, names=['word', 'frequency'])\n",
    "bg_wordlist = bg_wordlist.dropna()\n",
    "bg_wordlist = bg_wordlist[bg_wordlist.frequency > 2]\n",
    "bg_wordlist = bg_wordlist[bg_wordlist.word.str.contains(pat=r'[a-zA-Z]+')==False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_stopwords = pd.read_csv('https://raw.githubusercontent.com/Alir3z4/stop-words/master/bulgarian.txt', header=None, names=['word']).word\n",
    "en_stopwords = pd.Series(stopwords.words('english'))\n",
    "combined_stopwords = pd.concat([bg_stopwords, en_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_split(dataframe):\n",
    "    rows = list()\n",
    "    for row in dataframe[['sender_name', 'content', 'timestamp']].iterrows():\n",
    "        r = row[1]\n",
    "        for word in r.content:\n",
    "            rows.append((r.sender_name, word, r.timestamp))\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['sender_name', 'word', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_space = word_split(best_space_mess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_space = words_by_space[words_by_space.word.apply(len) >= 3]\n",
    "words_by_space.sender_name = words_by_space.sender_name.astype('category')\n",
    "words_by_space.word = words_by_space.word.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_words = words_by_space[(words_by_space.word.isin(bg_wordlist.word))\n",
    "                              & (words_by_space.word.isin(en_wordlist.word)==False)]\n",
    "\n",
    "en_words = words_by_space[(words_by_space.word.isin(en_wordlist.word)) \n",
    "                              & (words_by_space.word.isin(bg_wordlist.word)==False)]\n",
    "\n",
    "recycled_words = words_by_space[(words_by_space.word.isin(bg_wordlist.word)==False) \n",
    "                                    & (words_by_space.word.isin(en_wordlist.word)==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "recycled_words = recycled_words.copy()\n",
    "recycled_words.word = recycled_words.word.apply(translit_bg)\n",
    "recycled_words = recycled_words[recycled_words.word.isin(bg_wordlist.word)]\n",
    "bg_words = bg_words[bg_words.word.isin(bg_stopwords)==False]\n",
    "en_words = en_words[en_words.word.isin(en_stopwords)==False]\n",
    "recycled_words = recycled_words[recycled_words.word.isin(bg_stopwords)==False]\n",
    "bg_words.word = bg_words.word.apply(stem)\n",
    "en_words.word = en_words.word.apply(PorterStemmer().stem)\n",
    "recycled_words.word = recycled_words.word.apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words = pd.concat(objs=[bg_words, en_words, recycled_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mess = processed_words.groupby([\"sender_name\"]).word.agg(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mess = pd.DataFrame(processed_mess)\n",
    "processed_mess = processed_mess.reset_index(level=0).set_index('sender_name')\n",
    "processed_mess.columns = ['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=0.4, stop_words=combined_stopwords.tolist())\n",
    "X = vectorizer.fit_transform(raw_documents=processed_mess.content)\n",
    "X_norm = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "    n_clusters=16, n_init=10, n_jobs=8, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = n + 1\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=10, n_jobs=8)\n",
    "model.fit(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " –¥–æ–∫—Ç–æ—Ä\n",
      " –Ω–µ–∑–Ω–∞\n",
      " —â–æ—Ç\n",
      " –∞–¥–∏\n",
      " –º–∞–ª–µ\n",
      " –¥–µ—Ç\n",
      " –ø—Ä–∏–Ω—Ü–∏–ø\n",
      " –¥–∞—Å–∫–∞–ª\n",
      " —Ç–æ–µ—Å—Ç\n",
      " –π–µ—Å\n",
      " –¥–æ–æ–±\n",
      " —Å–º—è—Ç–∞\n",
      " –º–¥–∞–∞\n",
      " —Å–∏–≥\n",
      " –µ–µ–µ\n",
      " –º–∞–º–∫\n",
      " —Ç–∞–º—ä–Ω\n",
      " –∫–∏—Ç–∞—Ä\n",
      " –µ–ø–∏–∑–æ–¥\n",
      " –≤—ä–æ–±—â–µ\n",
      "Cluster 1:\n",
      " like\n",
      " know\n",
      " –ø—Ä–∏–Ω—Ü–∏–ø\n",
      " —Ç–æ–∑\n",
      " hate\n",
      " –Ω—è–∫–∞–∫–≤\n",
      " –æ–±–∏—á–∞–º\n",
      " fuck\n",
      " —Ç–∏—è\n",
      " love\n",
      " shit\n",
      " one\n",
      " kind\n",
      " want\n",
      " say\n",
      " –±–∞—Å–∏\n",
      " –±—É—Ä–≥–∞—Å\n",
      " –¥–æ–±\n",
      " –±—Ç–≤\n",
      " –º—Ä—ä–Ω–∫–∞\n",
      "Cluster 2:\n",
      " –¥–∞–µ\n",
      " —Ç—Ä—è\n",
      " —Ç–µ–π\n",
      " —Ç–∏—è\n",
      " –¥–∞–Ω–Ω\n",
      " –Ω—è–∫—ä–≤\n",
      " ama\n",
      " –∞–º–∏–∏\n",
      " dae\n",
      " –ø–æ–ª–∑–≤–∞\n",
      " —Å—Ö–µ–º\n",
      " —Å–∫–∏–≤\n",
      " —á–µ–∫\n",
      " sum\n",
      " –∫–≤–∏\n",
      " —É—Å–ª–æ–≤–∏\n",
      " —Ü–∏–∫—ä–ª\n",
      " –∫–æ–¥–∞\n",
      " che\n",
      " –ª–∞–π–Ω\n",
      "Cluster 3:\n",
      " ama\n",
      " sum\n",
      " ako\n",
      " –ø—É–∫\n",
      " ima\n",
      " —Ç—Ä—è–∞\n",
      " che\n",
      " –Ω–µ–∑–Ω–∞\n",
      " taka\n",
      " aha\n",
      " —â–æ\n",
      " ami\n",
      " tam\n",
      " imam\n",
      " —Ç–µ–±–µ\n",
      " sled\n",
      " –¥–µ—Å–µ\n",
      " —â–æ—Ç\n",
      " aid\n",
      " –∫—É–¥–µ\n",
      "Cluster 4:\n",
      " well\n",
      " like\n",
      " –¥–∞–µ\n",
      " –ª–æ–ª\n",
      " —â–æ—Ç\n",
      " –∏–¥–∫\n",
      " know\n",
      " —Ç—Ä—è–∞\n",
      " yeah\n",
      " —Ç–µ–±–µ\n",
      " –ø—Ä–∏–Ω—Ü–∏–ø\n",
      " get\n",
      " –Ω–∏–π\n",
      " –º–µ–Ω–µ\n",
      " —Ç–µ—è\n",
      " one\n",
      " –±–∞—Å–∏\n",
      " –Ω—è–∞\n",
      " –≤—ä–æ–±—â–µ\n",
      " –∑–Ω–∞–π—à\n",
      "Cluster 5:\n",
      " –∞—Ö–∞–º\n",
      " –Ω–µ–∑–Ω–∞\n",
      " –≤–∏–∫—Ç–æ—Ä\n",
      " —Å–º—è—Ç–∞\n",
      " —â–æ—Ç\n",
      " —Å–∏–≥\n",
      " –Ω–æ—â\n",
      " –±–∞—Ö—Ç\n",
      " ako\n",
      " –¥–æ–≤–µ—á–µ—Ä\n",
      " —Å—ä–±–æ\n",
      " —Ç—Ä—è—è\n",
      " ami\n",
      " –∫–æ–ª–∞\n",
      " –µ—Ö–µ–µ\n",
      " –¥–∞—Å–∫–∞–ª\n",
      " —Ç–æ–µ—Å—Ç\n",
      " –∞—Ö–∞–∞\n",
      " –¥–µ–Ω–∞\n",
      " –∫—ä—â–∏\n",
      "Cluster 6:\n",
      " –æ–∫–µ\n",
      " —Å–∏–≥\n",
      " –∏—Ä–∏–Ω\n",
      " —É–æ—É\n",
      " –¥–∞–∞\n",
      " —É—Ä–æ–∫\n",
      " –∏–¥–∫\n",
      " sum\n",
      " —Ñ–∞–º–∏–ª–∏\n",
      " che\n",
      " –ø–∏–µ–º\n",
      " —É–∞—É\n",
      " –≤–∏–Ω–æ\n",
      " –≥–æ—Ç–æ–≤–∞\n",
      " —Ç–∏—è\n",
      " —Ä–æ–∂–¥\n",
      " –º–µ–¥\n",
      " –Ω—è–∫–∞–∫–≤\n",
      " –∫—É–º\n",
      " –∑–≤—ä–Ω\n",
      "Cluster 7:\n",
      " –±–∞—Å–∏\n",
      " –Ω–∞–π—Å\n",
      " –∞–≤–µ—Ä\n",
      " –¥–∞–µ\n",
      " –µ—Ö–µ\n",
      " —â–æ—Ç\n",
      " –Ω—è–∞\n",
      " —Ç—Ä—è–∞\n",
      " haha\n",
      " —Ç–æ–µ—Å—Ç\n",
      " —Ñ–∞–∫\n",
      " —á–µ–∫\n",
      " –ø–∞–≤–∫\n",
      " –¥–µ—Ç\n",
      " nice\n",
      " —Å–∏–≥\n",
      " —Ö–∞—Ö–∞—Ö–∞\n",
      " —á–∏–ª\n",
      " dae\n",
      " —Ö–∞—Ö\n",
      "Cluster 8:\n",
      " —Å–µ–∫—Å\n",
      " –¥–∞–Ω–∏\n",
      " –∏–≤–∞–Ω\n",
      " –≥–µ–æ—Ä–≥\n",
      " —Ö–∞–π–¥\n",
      " –æ–±–∏—á–∞–º\n",
      " —Å–ª–∞–¥\n",
      " –ø–ª–∞–∂\n",
      " –¥–æ–±\n",
      " —Å—Ç–æ—è\n",
      " —Ü–µ–ª—É–≤–∫\n",
      " —Ö–∞—Ö–∞—Ö–∞\n",
      " –Ω–∞—à–∏\n",
      " —Å–ø—è\n",
      " –Ω–æ—â\n",
      " –±—Ç–≤\n",
      " –≤–∫—ä—â\n",
      " –Ω—É–∂–¥\n",
      " —Ü–µ–ª—É–≤–∞\n",
      " –≤–ª—é–±\n",
      "Cluster 9:\n",
      " like\n",
      " yeah\n",
      " well\n",
      " want\n",
      " know\n",
      " realli\n",
      " dont\n",
      " get\n",
      " –ª–µ–ª\n",
      " need\n",
      " one\n",
      " see\n",
      " time\n",
      " okay\n",
      " watch\n",
      " fuck\n",
      " feel\n",
      " talk\n",
      " think\n",
      " thing\n",
      "Cluster 10:\n",
      " –±–∞—Å–∏\n",
      " —Ç—Ä—è\n",
      " –¥–µ–π–±\n",
      " –¥–∞–µ\n",
      " –∏–¥–∫\n",
      " –Ω–µ—Å–∫\n",
      " —Ç–∏—è\n",
      " –∞–≤–≤\n",
      " –±–∞—è\n",
      " —â–æ—Ç\n",
      " fuck\n",
      " –Ω—è–∫—ä–≤\n",
      " —Å–ø—è\n",
      " —à–∏—Ç\n",
      " –∫–≤–æ—Ç\n",
      " –≤—Å—ä—â–Ω–æ—Å—Ç\n",
      " –∑–∞–∫–≤\n",
      " —Å–Ω–æ—â\n",
      " one\n",
      " like\n",
      "Cluster 11:\n",
      " dont\n",
      " like\n",
      " —â–æ—Ç\n",
      " well\n",
      " know\n",
      " fuck\n",
      " shi\n",
      " che\n",
      " –ª–µ–ª\n",
      " —Ç—Ä—è\n",
      " –∏–¥–∫\n",
      " –º–∞–Ω–¥–∂\n",
      " sum\n",
      " —Ñ–∞–∫\n",
      " get\n",
      " ama\n",
      " nice\n",
      " –ª–æ–ª\n",
      " yeah\n",
      " gonna\n",
      "Cluster 12:\n",
      " —Ç—Ä—è–∞\n",
      " well\n",
      " –Ω—è–∞\n",
      " –Ω–µ—Å–∫\n",
      " —É—á–∏–ª–∏—â\n",
      " —Ç–æ–∑\n",
      " –º–∞–º–∞\n",
      " –æ–±–∏—á–∞–º\n",
      " —Ñ—Ä–µ–Ω—Å–∫\n",
      " –µ–ª–µ–Ω\n",
      " —Ç–µ–∑\n",
      " –≤—Å—ä—â–Ω–æ—Å—Ç\n",
      " –∏–∑–æ–±—â–æ\n",
      " —Å–ª–∞–¥\n",
      " –Ω–æ—â\n",
      " –ø—Ä–∞–π—à\n",
      " –º–µ—Ö\n",
      " –±—Ç–≤\n",
      " –≤–∑–µ—Ç\n",
      " —Å–ø—è\n",
      "Cluster 13:\n",
      " che\n",
      " sum\n",
      " —â–æ—Ç\n",
      " ako\n",
      " ama\n",
      " taka\n",
      " imam\n",
      " men\n",
      " –¥–æ–±\n",
      " –æ–∫–µ\n",
      " —Å–ª–∞–¥\n",
      " sled\n",
      " ami\n",
      " –∏—Ä–∏–Ω\n",
      " toi\n",
      " tam\n",
      " ima\n",
      " –¥–µ–Ω–∞\n",
      " —Ç—Ä—è\n",
      " –Ω—è–∫–∞–∫–≤\n",
      "Cluster 14:\n",
      " —â–æ—Ç\n",
      " like\n",
      " –∏–¥–∫\n",
      " –¥–µ—Ç\n",
      " dont\n",
      " –Ω—è—è\n",
      " well\n",
      " get\n",
      " –±—Ç–≤\n",
      " one\n",
      " –±–∞—Å–∏\n",
      " fuck\n",
      " —Å–∏–≥\n",
      " know\n",
      " make\n",
      " –¥–∞–µ\n",
      " —Ç—Ä—è—è\n",
      " tho\n",
      " guy\n",
      " ill\n",
      "Cluster 15:\n",
      " like\n",
      " fuck\n",
      " well\n",
      " damn\n",
      " –∏–¥–∫\n",
      " god\n",
      " okay\n",
      " get\n",
      " know\n",
      " want\n",
      " thing\n",
      " realli\n",
      " feel\n",
      " yeah\n",
      " gonna\n",
      " even\n",
      " think\n",
      " make\n",
      " wut\n",
      " tell\n"
     ]
    }
   ],
   "source": [
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
